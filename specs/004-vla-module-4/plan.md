# Implementation Plan: Module 4 - Vision-Language-Action (VLA)

**Branch**: `004-vla-module-4` | **Date**: 2025-12-29 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/004-vla-module-4/spec.md`

## Summary

Create Module 4 documentation for the AI/Spec-Driven Book covering Vision-Language-Action (VLA) for voice-controlled humanoid robotics. The module includes three chapters: Voice-to-Action with Speech Recognition, LLM-Based Cognitive Planning, and a Capstone demonstrating end-to-end voice-controlled humanoid operation. All content is written as Markdown files for Docusaurus, following the established pattern from Modules 1-3.

## Technical Context

**Language/Version**: N/A (Documentation project - Markdown content)
**Primary Dependencies**: Docusaurus 3.x, Node.js 18+
**Storage**: N/A (Static documentation, no database)
**Testing**: Docusaurus build verification (`npm run build`)
**Target Platform**: GitHub Pages (static site)
**Project Type**: Documentation (Docusaurus)
**Performance Goals**: Build time < 60s, all examples verified against official docs
**Constraints**: Content must cite official OpenAI, ROS 2 documentation; word count 3000-5000 words
**Scale/Scope**: 3 chapters, approximately 3500-4500 words total, ~10 code examples

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Principle | Status | Notes |
|-----------|--------|-------|
| I. Spec-First Workflow | PASS | Spec created via `/sp.specify`, plan via `/sp.plan` |
| II. Technical Accuracy | PENDING | Will verify examples against official OpenAI/ROS 2 docs during implementation |
| III. Clear Developer-Focused Writing | PENDING | Will follow Module 1-3 writing patterns |
| IV. Reproducible Setup | PASS | Examples will be copy-paste ready with expected outputs |

**Gate Status**: PASS - No violations. Proceed with implementation.

## Project Structure

### Documentation (this feature)

```text
specs/004-vla-module-4/
├── plan.md              # This file
├── spec.md              # Feature specification
├── research.md          # Phase 0: Technology research
├── data-model.md        # Phase 1: Content structure
├── quickstart.md        # Phase 1: Implementation guide
├── contracts/           # Phase 1: Chapter contracts
│   └── chapter-structure.md
├── checklists/          # Quality validation
│   └── requirements.md
└── tasks.md             # Phase 2 output (via /sp.tasks)
```

### Source Code (Docusaurus content)

```text
frontend_book/
├── docs/
│   ├── module-1-ros2/           # Module 1 (existing)
│   ├── module-2-simulation/     # Module 2 (existing)
│   ├── module-3-nvidia-isaac/   # Module 3 (existing)
│   └── module-4-vla/            # Module 4 (this feature)
│       ├── _category_.json
│       ├── 01-voice-to-action.md
│       ├── 02-llm-cognitive-planning.md
│       └── 03-capstone-voice-humanoid.md
├── docusaurus.config.js
├── sidebars.js
└── package.json
```

**Structure Decision**: Documentation-only project following Module 1-3 pattern. All content in `frontend_book/docs/module-4-vla/` with autogenerated sidebar.

## Chapter Outline

### Chapter 1: Voice-to-Action with Speech Recognition

**File**: `frontend_book/docs/module-4-vla/01-voice-to-action.md`

**Learning Objectives**:
- Understand the VLA pipeline architecture
- Set up audio capture with voice activity detection
- Install and configure OpenAI Whisper for speech recognition
- Implement real-time speech-to-text transcription
- Publish transcribed commands to ROS 2 topics
- Handle speech recognition errors gracefully

**Key Sections**:
1. What You'll Learn
2. Introduction to Voice-Controlled Robotics
3. Speech Recognition Fundamentals
4. Setting Up Audio Capture
5. Implementing Whisper Transcription
6. Publishing to ROS 2
7. Key Takeaways

**Code Examples**:
- `audio_capture.py` - PyAudio microphone capture with VAD
- `whisper_node.py` - ROS 2 node wrapping Whisper transcription
- `voice_command_publisher.py` - Complete voice capture + publish node

**Target Word Count**: 1000-1500 words

---

### Chapter 2: LLM-Based Cognitive Planning

**File**: `frontend_book/docs/module-4-vla/02-llm-cognitive-planning.md`

**Learning Objectives**:
- Understand how LLMs serve as cognitive planners for robots
- Design effective system prompts for action generation
- Generate structured action plans in JSON format
- Parse LLM outputs into executable action primitives
- Map action primitives to ROS 2 action interfaces
- Implement safety validation for generated plans

**Key Sections**:
1. What You'll Learn
2. LLMs as Robot Planners
3. Prompt Engineering for Robotics
4. Generating Action Plans
5. Parsing and Validating LLM Output
6. Mapping Actions to ROS 2
7. Safety Considerations
8. Key Takeaways

**Code Examples**:
- `llm_planner_node.py` - ROS 2 node that calls LLM API
- `action_parser.py` - Parse LLM JSON output to action primitives
- `system_prompt.txt` - Example system prompt for robot planning
- `action_executor.py` - Execute parsed actions via ROS 2 action clients

**Target Word Count**: 1200-1800 words

---

### Chapter 3: Capstone - Voice-Controlled Humanoid

**File**: `frontend_book/docs/module-4-vla/03-capstone-voice-humanoid.md`

**Learning Objectives**:
- Integrate voice input, LLM planning, and robot execution
- Configure the complete VLA pipeline via launch files
- Execute navigation commands through voice
- Implement gesture execution via joint controllers
- Demonstrate multi-step autonomous task execution
- Test and validate the complete system

**Key Sections**:
1. What You'll Learn
2. Capstone Overview
3. Integrating the VLA Pipeline
4. Navigation via Voice Commands
5. Gesture Execution
6. Multi-Step Command Demo
7. Testing and Validation
8. Key Takeaways
9. Where to Go from Here

**Code Examples**:
- `vla_pipeline.launch.py` - Complete VLA launch file
- `gesture_action_server.py` - Simple gesture execution
- `capstone_demo.py` - End-to-end demonstration script

**Target Word Count**: 1000-1500 words

---

## Implementation Phases

### Phase 1: Setup (Infrastructure)
- Create `module-4-vla/` directory
- Create `_category_.json` metadata
- Create chapter file stubs with frontmatter
- Verify Docusaurus build passes

### Phase 2: Chapter 1 Content (P1 Priority)
- Write all sections for voice-to-action chapter
- Include 3 code examples with expected outputs
- Add hardware requirements (USB mic)
- Reference official Whisper documentation

### Phase 3: Chapter 2 Content (P2 Priority)
- Write all sections for LLM planning chapter
- Include 4 code examples with expected outputs
- Add API setup instructions
- Include safety consideration discussion

### Phase 4: Chapter 3 Content (P3 Priority)
- Write all sections for capstone chapter
- Include 3 code examples with expected outputs
- Integrate with Module 3 Nav2 setup
- Include multi-step demonstration

### Phase 5: Polish & Validation
- Verify word count (3000-5000 total)
- Verify build passes
- Check all internal links
- Validate code examples against official docs
- Review for technical accuracy

## Dependencies

### Module 1-3 Prerequisites
- ROS 2 fundamentals from Module 1
- Simulation basics from Module 2
- Nav2 navigation from Module 3 Chapter 3
- VSLAM perception from Module 3 Chapter 2

### External Documentation References
- [OpenAI Whisper](https://github.com/openai/whisper)
- [OpenAI API](https://platform.openai.com/docs)
- [ROS 2 Humble Actions](https://docs.ros.org/en/humble/Tutorials/Beginner-CLI-Tools/Understanding-ROS2-Actions.html)
- [Nav2 Documentation](https://navigation.ros.org/)
- [PyAudio](https://people.csail.mit.edu/hubert/pyaudio/)

## Risk Mitigation

| Risk | Mitigation |
|------|------------|
| OpenAI API changes | Use stable API version, document alternatives |
| Whisper model updates | Pin to specific version in examples |
| Word count constraints | Track word count per chapter during writing |
| Code example errors | Verify against official examples |
| Integration complexity | Build incrementally, test each component |

## Success Metrics

From spec success criteria:
- SC-001: Voice capture in 5 minutes ✓
- SC-002: >90% accuracy for simple commands ✓
- SC-003: Action plans in 10 minutes ✓
- SC-004: Plans map to valid ROS 2 actions ✓
- SC-005: Capstone demo in 20 minutes ✓
- SC-006: VLA architecture understanding ✓
- SC-007: All examples run successfully ✓
- SC-008: Explain 3 LLM-robotics challenges ✓

## Artifacts Generated

| Artifact | Path | Purpose |
|----------|------|---------|
| Research | `specs/004-vla-module-4/research.md` | Technology decisions |
| Data Model | `specs/004-vla-module-4/data-model.md` | Content structure |
| Contracts | `specs/004-vla-module-4/contracts/` | Chapter requirements |
| Quickstart | `specs/004-vla-module-4/quickstart.md` | Implementation guide |
| Plan | `specs/004-vla-module-4/plan.md` | This file |

## Next Steps

1. Run `/sp.tasks` to generate actionable task list
2. Execute tasks via `/sp.implement`
3. Verify build passes and word count
4. Create PR for review
